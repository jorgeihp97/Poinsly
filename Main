from sly import Lexer, Parser
import cmath

class PoinLexer(Lexer):
    tokens = { NUMBER, ADD, MUL, LPAREN, RPAREN, TOKEN_X }
    ignore = ' \t'

    # Tokens
    ADD = r'\+'
    MUL = r'\*'
    LPAREN = r'\('
    RPAREN = r'\)'
    TOKEN_X = r'[xX]'

    @_(r'\d+')
    def NUMBER(self, t):
        t.value = int(t.value)
        return t

    @_(r'\n+')
    def newline(self, t):
        self.lineno += t.value.count('\n')

    def error(self, t):
        print("Illegal character '%s'" % t.value[0])
        self.index += 1

class PoinParser(Parser):
    debugfile = 'Pparser.txt'
    tokens = PoinLexer.tokens

    precedence = (
        ('left', 'TOKEN_X', 'ADD'),
        ('left', 'MUL'),
        )

    def __init__(self):
        self.names = { }

    @_('number')
    def statement(self, p):
        print(p.number)
    
   #Intermediate Code v
    @_('LPAREN number "," number RPAREN "," LPAREN number "," number RPAREN') 
    def number(self, p):
        a = (p.number3 - p.number1) / (p.number2 - p.number0)
        b = p.number1 - (a * p.number0)
        return a,b

    @_('linear_term')
    def number(self, p):
        return p.linear_term

    @_('quadratic_term')
    def number(self, p):
        return p.quadratic_term

    @_('number TOKEN_X')
    def linear_term(self, p):
        return p.number

    @_('number TOKEN_X MUL MUL "2"')
    def quadratic_term(self, p):
        return p.number
    
    @_('quadratic_term ADD linear_term ADD number') 
    def number(self, p):
        c = (p.linear_term ** 2) - (4 * p.quadratic_term * p.number0)
        a = (-p.linear_term - cmath.sqrt(c))/(2*p.quadratic_term)
        b = (-p.linear_term - cmath.sqrt(c))/(2*p.quadratic_term)
        return a , b
    #Intermediate Code ^
    @_('NUMBER')
    def number(self, p):
        return p.NUMBER

if __name__ == '__main__':
    lexer = PoinLexer()
    parser = PoinParser()
    print('Poinsly Language\n')
    while True:
        try:
            text = input('--> ')
        except EOFError:
            break
        if text:
            parser.parse(lexer.tokenize(text))
